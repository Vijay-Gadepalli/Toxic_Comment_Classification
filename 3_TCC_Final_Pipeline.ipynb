{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Case_Study_1_Final_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp3j0UGkgUTt"
      },
      "source": [
        "# Toxic Comment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlrPaF-YhAMw"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Yn7yxtL2nnl",
        "outputId": "a74dfe18-bfc3-41fd-e012-3f45ab5b905d"
      },
      "source": [
        "# contractions for pre-processing\n",
        "!pip install contractions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/11/4d/378ab91284c2c3a06ab475b287721c09b7951d5ecb3edf4ffb0e1e7a568a/contractions-0.0.49-py2.py3-none-any.whl\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/fe/021d7d76961b5ceb9f8d022c4138461d83beff36c3938dc424586085e559/textsearch-0.0.21-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7f/c2/eae730037ae1cbbfaa229d27030d1d5e34a1e41114b21447d1202ae9c220/pyahocorasick-1.4.2.tar.gz (321kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 6.7MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/14/666cd44bf53f36a961544af592cb5c5c800013f9c51a4745af8d7c17362a/anyascii-0.2.0-py3-none-any.whl (283kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 37.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85397 sha256=66ab9010149cdbc962239355bbf500dea9172590cb44c83c7506faaa8f378dfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/03/34/77e3ece0bba8b86bfac88a79f923b36d805cad63caeba38842\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.2.0 contractions-0.0.49 pyahocorasick-1.4.2 textsearch-0.0.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uugCULin6SHm"
      },
      "source": [
        "#mandatory libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "\n",
        "#nltk-preprocessing\n",
        "import string\n",
        "import nltk\n",
        "import contractions\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "\n",
        "#misc\n",
        "import re\n",
        "import pickle\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from collections.abc import Iterable\n",
        "\n",
        "#metrics\n",
        "from sklearn.metrics import hamming_loss\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import roc_curve, auc,roc_auc_score\n",
        "\n",
        "#model loading\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KckCgd9NaSEe",
        "outputId": "ad7b012c-5e1b-4b6c-ad3f-5b35baa297a6"
      },
      "source": [
        "#supporting/essential downloads for NLTK library \n",
        "#to handle chuncking/stemming/stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzlBHPM-jGQb"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "205GCh-1jOGS",
        "outputId": "87ed27ef-af45-4cce-c792-a40e2c4168fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPVwcn2tmhzL"
      },
      "source": [
        "## Pre-Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsKH4TXZbu8Z"
      },
      "source": [
        "def convert_to_lower_case(text):\n",
        "\n",
        "    \"\"\"function to convert the input text to lower case\"\"\"\n",
        "    \n",
        "    return text.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NU-N6HAdThD"
      },
      "source": [
        "def remove_escape_char(text):\n",
        "\n",
        "    \"\"\"function to remove newline (\\n),\n",
        "    tab(\\t) and slashes (/ , \\) from the input text\"\"\"\n",
        "\n",
        "    return re.sub(r\"[\\n\\t\\\\\\/]\",\" \",text, flags=re.MULTILINE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzmvWtuSZQsT"
      },
      "source": [
        "def remove_html_tags(text):\n",
        "\n",
        "    \"\"\"function to remove html tags (< >) and its content \n",
        "    from the input text\"\"\"\n",
        "\n",
        "    return re.sub(r\"<.*>\",\" \",text, flags=re.MULTILINE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2BvBeJeZkAp"
      },
      "source": [
        "def remove_links(text):\n",
        "    \"\"\"function to remove any kind of links with no \n",
        "    html tags\"\"\"\n",
        "\n",
        "    text= re.sub(r\"http\\S+\",\" \",text, flags=re.MULTILINE)\n",
        "\n",
        "    return re.sub(r\"www\\S+\",\" \",text, flags=re.MULTILINE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIHBfCGba78L"
      },
      "source": [
        "def remove_digits(text):\n",
        "\n",
        "    \"\"\"function to remove digits from the input text\"\"\"\n",
        "\n",
        "    return re.sub(r'\\d',\" \",text, flags=re.MULTILINE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ET_UA6fecv-"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "\n",
        "    \"\"\"function to remove punctuation marks from the input text\"\"\"\n",
        "\n",
        "    for i in string.punctuation:\n",
        "        text = text.replace(i,\" \")\n",
        "\n",
        "    return text      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJ-FhpYS9e6_"
      },
      "source": [
        "def chuncking(text):\n",
        "\n",
        "    \"\"\"function to perform chucking, which is also referred as shallow parsing.\n",
        "    This is useful in determing the parts of speech of a given text and adds more\n",
        "    structure to the input data .\"\"\"\n",
        "\n",
        "    \"\"\"In this function, we use NLTK library to perform chuncking and if a \n",
        "    particular label is PERSON names, we remove that, and names of Geo-graphic\n",
        "    ares are retained by adding _ in its words.ex-New_York\"\"\"\n",
        "\n",
        "\n",
        "    chunks_data=[]\n",
        "    chunks_data=(list(ne_chunk(pos_tag(word_tokenize(text)))))\n",
        "    for label in chunks_data:\n",
        "        if type(label)==Tree:\n",
        "            if label.label() == \"GPE\":\n",
        "                a = label.leaves()\n",
        "                if len(a)>1:\n",
        "                    gpe = \"_\".join([term for term,pos in a])\n",
        "                    text = re.sub(rf'{a[1][0]}',gpe,text, flags=re.MULTILINE)\n",
        "                    text = re.sub(rf'\\b{a[0][0]}\\b',\" \",text, flags=re.MULTILINE)\n",
        "            if label.label()==\"PERSON\":      \n",
        "                for term,pog in label.leaves():\n",
        "                    text = re.sub(re.escape(term),\" \",text, flags=re.MULTILINE)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4vTAR7vxnsj"
      },
      "source": [
        "def keep_alpha_and_underscore(text):\n",
        "\n",
        "    \"\"\"function to keep only aphabets and _ underscore, as we \n",
        "    added it in the chunking for geographic locations.\"\"\"\n",
        "    \n",
        "    return re.sub(r\"[^a-zA-Z_]\",\" \",text,flags=re.MULTILINE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56yqoqV3Yo0b"
      },
      "source": [
        "def remove_extra_spaces_if_any(text):\n",
        "\n",
        "    \"\"\"function to remove extra spaces if any after all the pre-preocessing\"\"\"\n",
        "    \n",
        "    return re.sub(r\" {2,}\", \" \", text, flags=re.MULTILINE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3nhadvmTaJA"
      },
      "source": [
        "def remove_repeated_characters(text):\n",
        "\n",
        "    \"\"\"function to remove repeated characters if any from the input text\"\"\"\n",
        "\n",
        "    \"\"\"for example CAAAAASSSSSSEEEEE SSSSTTTTTUUUUUUDDDDYYYYYY gives CASE STUDY\"\"\"\n",
        "\n",
        "    return re.sub(r\"(\\w)(\\1{2,})\",\"\\\\1\",text,flags=re.MULTILINE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74owF4Rzc6_q"
      },
      "source": [
        "def remove_words_lesth2(text):\n",
        "    \"\"\"function to remove words with length less than 2\"\"\"\n",
        "\n",
        "    text = re.sub(r'\\b\\w{1,2}\\b',\" \",text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k92koecvSugv"
      },
      "source": [
        "def decontraction(text):\n",
        "\n",
        "    \"\"\"function to handle contraction errors\"\"\"\n",
        "    res=\"\"\n",
        "    for word in text.split():\n",
        "        try:\n",
        "            con_text=contractions.fix(word)\n",
        "            if con_text.lower() is word.lower():\n",
        "                res=res+word+\" \"\n",
        "            else:\n",
        "                res=res+con_text+\" \"\n",
        "        \n",
        "        except:\n",
        "            con_text=contractions.fix(word.lower())\n",
        "            if con_text.lower() is word.lower():\n",
        "                res=res+word+\" \"\n",
        "            else:\n",
        "                res=res+con_text+\" \"\n",
        "    return res.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF5KwSg1YuAV"
      },
      "source": [
        "#lets take all the stop words from both NLTK & Word Cloud libraries, along \n",
        "# with some custom words\n",
        "\n",
        "stop_words=stopwords.words('english')\n",
        "word_cloud_stp_wrds=list(STOPWORDS)\n",
        "final_stop_words=list(STOPWORDS.union(set(stop_words)))\n",
        "final_stop_words.extend([\"mr\",\"mrs\",\"miss\",\n",
        "                        \"one\",\"two\",\"three\",\"four\",\"five\",\n",
        "                        \"six\",\"seven\",\"eight\",\"nine\",\"ten\",\n",
        "                        \"us\",\"also\",\"dont\",\"cant\",\"any\",\"can\",\"along\",\n",
        "                        \"among\",\"during\",\"anyone\",\n",
        "                         \"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\n",
        "                         \"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"hi\",\"hello\",\"hey\",\"ok\",\n",
        "                         \"okay\",\"lol\",\"rofl\",\"hola\",\"let\",\"may\",\"etc\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un1DIpXKdJlW"
      },
      "source": [
        "#lemmatizer object\n",
        "lemmatiser = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwPljMm8aSx7"
      },
      "source": [
        "# one-step pre-processing function\n",
        "\n",
        "def preprocess(text):\n",
        "\n",
        "    preprocessed_text = []\n",
        "\n",
        "    for each_text in text:\n",
        "\n",
        "        result=remove_links(each_text)\n",
        "        result=remove_html_tags(result)\n",
        "        result=remove_escape_char(result)        \n",
        "        result=remove_digits(result)\n",
        "        result=decontraction(result)\n",
        "        result=remove_punctuation(result)\n",
        "        result=chuncking(result)\n",
        "        result=convert_to_lower_case(result)\n",
        "        result = ' '.join(non_stop_word for non_stop_word in result.split() if non_stop_word not in final_stop_words)\n",
        "        result=keep_alpha_and_underscore(result)\n",
        "        result=remove_extra_spaces_if_any(result)\n",
        "        result=remove_repeated_characters(result)\n",
        "        result=remove_words_lesth2(result)\n",
        "        result=' '.join(lemmatiser.lemmatize(word,pos=\"v\") for word in result.split())\n",
        "        preprocessed_text.append(result.strip())\n",
        "        \n",
        "    return preprocessed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Oh__GSj__Gs"
      },
      "source": [
        "## Featurization Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0VkMHg6FwV3"
      },
      "source": [
        "#load data\n",
        "\n",
        "tfidf_dict = joblib.load('/content/gdrive/MyDrive/Colab Notebooks/Case_Study_1/tfidf_dict.pkl')\n",
        "tfidf_words = joblib.load('/content/gdrive/MyDrive/Colab Notebooks/Case_Study_1/tfidf_words.pkl')\n",
        "w2v_dict = joblib.load('/content/gdrive/MyDrive/Colab Notebooks/Case_Study_1/w2v_dict.pkl')\n",
        "w2v_words = joblib.load('/content/gdrive/MyDrive/Colab Notebooks/Case_Study_1/w2v_words.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-AxloST437R"
      },
      "source": [
        "# computing tf-idf weighted word2vec for each comment.\n",
        "\n",
        "def comp_tfidf_weighted_w2v(data,w2v_words,tfidf_words,w2v_dict,tfidf_dict):\n",
        "\n",
        "    tfidf_w2v = []\n",
        "    for sentence in data:\n",
        "        vector = np.zeros(300) \n",
        "        # as word vectors are of zero length\n",
        "        tf_idf_weight =0;\n",
        "        # num of words with a valid vector in the sentence/review\n",
        "        for word in sentence.split(): \n",
        "            # for each word in a review/sentence\n",
        "            if (word in w2v_words) and (word in tfidf_words):\n",
        "                vec = w2v_dict[word] \n",
        "                # getting the vector for each word\n",
        "                # here we are multiplying idf value(dictionary[word]) and \n",
        "                #the tf value((sentence.count(word)/len(sentence.split())))\n",
        "                tf_idf = tfidf_dict[word]*(sentence.count(word)/len(sentence.split()))\n",
        "                # getting the tfidf value for each word\n",
        "                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
        "                tf_idf_weight += tf_idf\n",
        "        if tf_idf_weight != 0:\n",
        "            vector /= tf_idf_weight\n",
        "        tfidf_w2v.append(vector)\n",
        "    return np.array(tfidf_w2v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJNcYEDxGxMB"
      },
      "source": [
        "## Loading Model & Function for Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDG_8x9gG0gY"
      },
      "source": [
        "#loading model\n",
        "model=load_model(\"/content/gdrive/MyDrive/Colab Notebooks/Case_Study_1/mlp_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Afb1MDrXezp9"
      },
      "source": [
        "#saving into hdf5 format\n",
        "model.save(\"/content/gdrive/MyDrive/Colab Notebooks/Case_Study_1/best_model.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-N1dRjctLiZD"
      },
      "source": [
        "def cal_metrics(y_true,y_pred):\n",
        "\n",
        "    \"\"\"function to calculate final metrics \"\"\"\n",
        "\n",
        "    if isinstance(y_true,scipy.sparse.lil.lil_matrix):\n",
        "        y_true=y_true.A\n",
        "    \n",
        "    if isinstance(y_pred,scipy.sparse.lil.lil_matrix):\n",
        "        y_pred=y_pred.A\n",
        "\n",
        "    acc=accuracy_score(y_true,y_pred)\n",
        "    ham_loss=hamming_loss(y_true,y_pred)\n",
        "\n",
        "    return {\"Accuracy\":acc,\"Hamming Loss\":ham_loss}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8r4yi3O7Cjz"
      },
      "source": [
        "## Function-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxOy-8B07BJn"
      },
      "source": [
        "def function_1(X):\n",
        "\n",
        "    #handling single & multiple inputs\n",
        "\n",
        "    if isinstance(X,str):\n",
        "        X=[X]\n",
        "\n",
        "    elif isinstance(X,Iterable):\n",
        "        X=X\n",
        "\n",
        "    #pre-processing\n",
        "    pp_text=preprocess(X)\n",
        "\n",
        "    #vectorizing\n",
        "    vect_data=comp_tfidf_weighted_w2v(pp_text,w2v_words,\n",
        "                                      tfidf_words,\n",
        "                                      w2v_dict,\n",
        "                                      tfidf_dict)\n",
        "    pred=model.predict(vect_data).round().astype(int)\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epCPIX8PDFGC"
      },
      "source": [
        "## Function-2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNID5rb_8ac"
      },
      "source": [
        "def function_2(X,y):\n",
        "\n",
        "    #handling single & multiple inputs\n",
        "\n",
        "    if isinstance(X,str):\n",
        "        X=[X]\n",
        "\n",
        "    elif isinstance(X,Iterable):\n",
        "        X=X\n",
        "\n",
        "    #pre-processing\n",
        "    pp_text=preprocess(X)\n",
        "\n",
        "    #vectorizing\n",
        "    vect_data=comp_tfidf_weighted_w2v(pp_text,w2v_words,\n",
        "                                      tfidf_words,\n",
        "                                      w2v_dict,\n",
        "                                      tfidf_dict)\n",
        "\n",
        "    pred=model.predict(vect_data).round().astype(int)\n",
        "    \n",
        "    metrics=[]\n",
        "    for ground,predct in zip(y,pred):\n",
        "        d=cal_metrics(ground,predct)\n",
        "        metrics.append(list(d.values()))\n",
        "    \n",
        "    return pd.DataFrame(data=metrics,columns=[\"Exact Match Ratio\",\"Hamming Loss\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ6SuVI6H6aQ",
        "outputId": "86a56d4a-fd2d-4efd-9797-f6c21bb872f4"
      },
      "source": [
        "function_1([\"this is a final submisssion for prection function. please give proper output\",\n",
        "            \"keep mask follow social distance\",\n",
        "            \"stay home stay safe\",\n",
        "            \"are you mad ? i will kill you if you ask again\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "3MkQpfxuMR5y",
        "outputId": "4e8a8bdf-ad51-4fd7-935f-b30486dfbad8"
      },
      "source": [
        "function_2([\"this is a final submisssion for prection function. please give proper output\",\n",
        "            \"keep mask follow social distance\",\n",
        "            \"stay home stay safe\",\n",
        "            \"are you mad or what ? i will kill you if you ask again\"],\n",
        "           [[0,0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0],[0,0,0,0,0,0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Exact Match Ratio</th>\n",
              "      <th>Hamming Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.166667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Exact Match Ratio  Hamming Loss\n",
              "0           1.000000      0.000000\n",
              "1           1.000000      0.000000\n",
              "2           1.000000      0.000000\n",
              "3           0.833333      0.166667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    }
  ]
}